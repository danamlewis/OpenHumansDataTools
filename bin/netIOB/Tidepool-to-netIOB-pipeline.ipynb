{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdafeb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#########################################################\n",
    "#\n",
    "#   1) Ingest and clean data from two sheets (basal and bolus)\n",
    "#      in the Excel file \"NAMED-FILE.xlsx\" Obviously, rename this file below!\n",
    "#   2) Chunk basal data into flexible intervals (each <=6 minutes),\n",
    "#      computing the delivered insulin \"Amount\" (rounded to 3 decimals)\n",
    "#      and carrying the original Rate as \"Temp rate\"\n",
    "#   3) Merge bolus data (left as-is) with the chunked basal data and export \n",
    "#      the merged DataFrame to a CSV file.\n",
    "#\n",
    "############################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from datetime import timedelta\n",
    "\n",
    "##############################\n",
    "# 1) Ingest & Clean the Data\n",
    "##############################\n",
    "\n",
    "# Specify the sheet names for basal and bolus data:\n",
    "basal_sheet_name = \"Basal\"   # replace with your actual basal sheet name\n",
    "bolus_sheet_name = \"Bolus\"   # replace with your actual bolus sheet name\n",
    "\n",
    "# Read basal data from the specified sheet\n",
    "basal_df = pd.read_excel(\"NAMED-FILE.xlsx\", \n",
    "                         sheet_name=basal_sheet_name)\n",
    "\n",
    "# Read bolus data from the specified sheet\n",
    "bolus_df = pd.read_excel(\"NAMED-FILE.xlsx\", \n",
    "                         sheet_name=bolus_sheet_name)\n",
    "\n",
    "# For basal, keep these columns:\n",
    "# Local Time, Tidepool Data Type, Duration (mins), Rate,\n",
    "# Suppressed Delivery Type, Suppressed Type, Suppressed Rate\n",
    "basal_cols = [\n",
    "    \"Local Time\",\n",
    "    \"Tidepool Data Type\",\n",
    "    \"Duration (mins)\",\n",
    "    \"Rate\",\n",
    "    \"Suppressed Delivery Type\",\n",
    "    \"Suppressed Type\",\n",
    "    \"Suppressed Rate\"\n",
    "]\n",
    "basal_df = basal_df[basal_cols].copy()\n",
    "\n",
    "# For bolus, keep these columns:\n",
    "# Local Time, Tidepool Data Type, Normal (the bolus amount)\n",
    "bolus_cols = [\n",
    "    \"Local Time\",\n",
    "    \"Tidepool Data Type\",\n",
    "    \"Normal\"\n",
    "]\n",
    "bolus_df = bolus_df[bolus_cols].copy()\n",
    "\n",
    "# Convert Local Time to datetime for both DataFrames\n",
    "basal_df[\"Local Time\"] = pd.to_datetime(basal_df[\"Local Time\"], errors=\"coerce\")\n",
    "bolus_df[\"Local Time\"] = pd.to_datetime(bolus_df[\"Local Time\"], errors=\"coerce\")\n",
    "\n",
    "# Convert numeric columns appropriately\n",
    "basal_df[\"Duration (mins)\"] = pd.to_numeric(basal_df[\"Duration (mins)\"], errors=\"coerce\")\n",
    "basal_df[\"Rate\"] = pd.to_numeric(basal_df[\"Rate\"], errors=\"coerce\")\n",
    "basal_df[\"Suppressed Rate\"] = pd.to_numeric(basal_df[\"Suppressed Rate\"], errors=\"coerce\")\n",
    "bolus_df[\"Normal\"] = pd.to_numeric(bolus_df[\"Normal\"], errors=\"coerce\")\n",
    "\n",
    "# Display initial rows for sanity check\n",
    "print(\"Basal Data (first 3 rows):\")\n",
    "display(basal_df.head(3))\n",
    "print(\"\\nBolus Data (first 3 rows):\")\n",
    "display(bolus_df.head(3))\n",
    "\n",
    "###############################\n",
    "# 2) Chunk Basal Data with Flexible Duration (<=6 minutes)\n",
    "###############################\n",
    "\n",
    "def chunk_basal_flexible(df_basal, max_chunk_duration=6):\n",
    "    \"\"\"\n",
    "    Splits each basal record into evenly sized chunks such that each chunk's duration \n",
    "    is less than or equal to max_chunk_duration minutes.\n",
    "    \n",
    "    For each basal record:\n",
    "      - Let D = Duration (mins)\n",
    "      - Determine number of chunks n = ceil(D / max_chunk_duration)\n",
    "      - Each chunk will have duration = D / n minutes (which is <= max_chunk_duration)\n",
    "      - The delivered insulin \"Amount\" in each chunk is:\n",
    "          (Rate in U/hr / 60) * (chunk duration)\n",
    "        and rounded to 3 decimals.\n",
    "      - Also, the original Rate is carried over as \"Temp rate\" for validation.\n",
    "      \n",
    "    Returns a DataFrame with the following columns:\n",
    "      - Local Time: start time of the chunk (increments by chunk duration)\n",
    "      - Tidepool Data Type: set to \"basal\"\n",
    "      - Duration (mins): duration of that chunk (may be fractional)\n",
    "      - Amount: insulin units delivered in that chunk (rounded to 3 decimals)\n",
    "      - Temp rate: the original Rate from the basal record\n",
    "      - Suppressed Delivery Type, Suppressed Type, Suppressed Rate (carried over)\n",
    "    \"\"\"\n",
    "    chunked_rows = []\n",
    "\n",
    "    for idx, row in df_basal.iterrows():\n",
    "        start_time = row[\"Local Time\"]\n",
    "        total_duration = row[\"Duration (mins)\"]\n",
    "        hourly_rate = row[\"Rate\"]  # in U/hr\n",
    "        \n",
    "        # Additional columns to carry over\n",
    "        suppressed_delivery_type = row[\"Suppressed Delivery Type\"]\n",
    "        suppressed_type = row[\"Suppressed Type\"]\n",
    "        suppressed_rate = row[\"Suppressed Rate\"]\n",
    "\n",
    "        # Skip record if duration is invalid\n",
    "        if pd.isna(total_duration) or total_duration <= 0:\n",
    "            continue\n",
    "\n",
    "        # Determine number of chunks (n) such that each chunk's duration <= max_chunk_duration\n",
    "        n_chunks = ceil(total_duration / max_chunk_duration)\n",
    "        # Calculate the actual chunk duration (in minutes)\n",
    "        chunk_duration = total_duration / n_chunks\n",
    "\n",
    "        # Calculate rate in U per minute (if rate is available)\n",
    "        rate_per_minute = np.nan\n",
    "        if not pd.isna(hourly_rate):\n",
    "            rate_per_minute = hourly_rate / 60.0\n",
    "\n",
    "        # Create each chunk row\n",
    "        for i in range(n_chunks):\n",
    "            # Calculate start time for this chunk\n",
    "            chunk_start = start_time + timedelta(minutes=i * chunk_duration)\n",
    "            # Calculate the amount delivered in this chunk\n",
    "            chunk_amount = np.nan\n",
    "            if not pd.isna(rate_per_minute):\n",
    "                chunk_amount = round(rate_per_minute * chunk_duration, 3)\n",
    "\n",
    "            new_row = {\n",
    "                \"Local Time\": chunk_start,\n",
    "                \"Tidepool Data Type\": \"basal\",\n",
    "                \"Duration (mins)\": round(chunk_duration, 3),\n",
    "                \"Amount\": chunk_amount,\n",
    "                \"Temp rate\": hourly_rate,\n",
    "                \"Suppressed Delivery Type\": suppressed_delivery_type,\n",
    "                \"Suppressed Type\": suppressed_type,\n",
    "                \"Suppressed Rate\": suppressed_rate\n",
    "            }\n",
    "            chunked_rows.append(new_row)\n",
    "\n",
    "    chunked_df = pd.DataFrame(chunked_rows)\n",
    "    chunked_df.sort_values(by=\"Local Time\", inplace=True)\n",
    "    chunked_df.reset_index(drop=True, inplace=True)\n",
    "    return chunked_df\n",
    "\n",
    "# Apply the flexible basal chunking function (max_chunk_duration set to 6 minutes)\n",
    "chunked_basal_df = chunk_basal_flexible(basal_df, max_chunk_duration=6)\n",
    "\n",
    "print(\"\\nChunked Basal Data (first 5 rows):\")\n",
    "display(chunked_basal_df.head(5))\n",
    "print(\"Number of rows in chunked basal:\", len(chunked_basal_df))\n",
    "\n",
    "###########################################\n",
    "# 3) Merge Chunked Basal Data with Bolus Data\n",
    "###########################################\n",
    "\n",
    "# Final unified columns:\n",
    "# [Local Time, Tidepool Data Type, Duration (mins), Amount, Temp rate,\n",
    "#  Suppressed Delivery Type, Suppressed Type, Suppressed Rate]\n",
    "\n",
    "final_cols = [\n",
    "    \"Local Time\",\n",
    "    \"Tidepool Data Type\",\n",
    "    \"Duration (mins)\",\n",
    "    \"Amount\",\n",
    "    \"Temp rate\",\n",
    "    \"Suppressed Delivery Type\",\n",
    "    \"Suppressed Type\",\n",
    "    \"Suppressed Rate\"\n",
    "]\n",
    "\n",
    "# For bolus data, rename \"Normal\" to \"Amount\" and leave the other fields blank.\n",
    "bolus_renamed = bolus_df.rename(columns={\"Normal\": \"Amount\"}).copy()\n",
    "bolus_renamed[\"Duration (mins)\"] = \"\"\n",
    "bolus_renamed[\"Temp rate\"] = \"\"\n",
    "bolus_renamed[\"Suppressed Delivery Type\"] = \"\"\n",
    "bolus_renamed[\"Suppressed Type\"] = \"\"\n",
    "bolus_renamed[\"Suppressed Rate\"] = \"\"\n",
    "\n",
    "# Ensure both DataFrames have the same columns in the same order\n",
    "bolus_renamed = bolus_renamed[final_cols]\n",
    "chunked_basal_df = chunked_basal_df[final_cols]\n",
    "\n",
    "# Concatenate and sort by Local Time\n",
    "merged_df = pd.concat([chunked_basal_df, bolus_renamed], ignore_index=True)\n",
    "merged_df.sort_values(by=\"Local Time\", inplace=True)\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"\\nMerged Basal & Bolus Data (first 10 rows):\")\n",
    "display(merged_df.head(10))\n",
    "print(\"Total rows in merged DataFrame:\", len(merged_df))\n",
    "\n",
    "##################################\n",
    "# EXPORT THE MERGED DATA TO CSV\n",
    "##################################\n",
    "# Use na_rep='' so that blank cells appear as empty strings.\n",
    "output_file = \"insulin_5min_merged-input-netIOB.csv\"\n",
    "merged_df.to_csv(output_file, index=False, na_rep='')\n",
    "\n",
    "print(f\"\\nDone! Exported final merged data to '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016def0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING BASAL PROFILES FOR EACH DAY\n",
    "\"\"\"\n",
    "Methods – Generation of Daily Basal Insulin Profiles\n",
    "\n",
    "The merged insulin dataset was first filtered to extract basal insulin records, defined by the \"Tidepool Data Type\" field.\n",
    "From these basal records, the date and hour components were extracted from the \"Local Time\" column. For each day, records\n",
    "were grouped by hour (0–23) and the average \"Suppressed Rate\" was computed for each hour after excluding missing values.\n",
    "To ensure a complete hourly profile, missing hourly values were imputed using forward‐ and backward‐filling.\n",
    "The resulting hourly averages were then organized into a structured JSON object; each element of the basal profile array \n",
    "includes an index, a start time (formatted as \"HH:00:00\"), the cumulative minute offset, and the computed average insulin \n",
    "delivery rate (in IU/hr). Daily profiles were saved as separate JSON files, providing reproducible and scalable inputs \n",
    "for subsequent netIOB analyses.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea4ca68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# -------------------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------------------\n",
    "merged_file = \"insulin_5min_merged-input-netIOB.csv\"\n",
    "profiles_folder = \"daily_profiles\"\n",
    "os.makedirs(profiles_folder, exist_ok=True)\n",
    "\n",
    "# -------------------------------------------\n",
    "# READ DATA & FILTER BASAL RECORDS\n",
    "# -------------------------------------------\n",
    "insulin_df = pd.read_csv(merged_file, parse_dates=[\"Local Time\"])\n",
    "# Filter only basal rows (case-insensitive check)\n",
    "basal_df = insulin_df[insulin_df[\"Tidepool Data Type\"].str.lower() == \"basal\"].copy()\n",
    "\n",
    "# -------------------------------------------\n",
    "# ADD Date and Hour COLUMNS\n",
    "# -------------------------------------------\n",
    "basal_df[\"Date\"] = basal_df[\"Local Time\"].dt.date\n",
    "basal_df[\"Hour\"] = basal_df[\"Local Time\"].dt.hour\n",
    "\n",
    "# Dictionary to store the filled profiles for each day (for later inspection if needed)\n",
    "daily_profiles = {}\n",
    "\n",
    "# -------------------------------------------\n",
    "# PROCESS EACH DAY: Compute hourly averages (ignoring NaNs) for each day in the dataset\n",
    "# -------------------------------------------\n",
    "for day, group in basal_df.groupby(\"Date\"):\n",
    "    print(f\"\\nProcessing day: {day}\")\n",
    "    \n",
    "    # Compute the average \"Suppressed Rate\" per hour, ignoring NaN values.\n",
    "    hourly_profile = group.groupby(\"Hour\")[\"Suppressed Rate\"]\\\n",
    "                          .apply(lambda x: x.dropna().mean())\\\n",
    "                          .to_dict()\n",
    "    \n",
    "    # Build a template for all 24 hours (0 to 23). Missing hours will be None.\n",
    "    profile_dict = {hr: hourly_profile.get(hr, None) for hr in range(24)}\n",
    "    \n",
    "    # Debug: Print out all suppressed rate values and the computed average for each hour.\n",
    "    for hr in range(24):\n",
    "        # Get all raw suppressed rate values for this hour\n",
    "        values = group[group[\"Hour\"] == hr][\"Suppressed Rate\"].tolist()\n",
    "        valid_values = [v for v in values if pd.notna(v)]\n",
    "        avg_val = profile_dict[hr]\n",
    "        print(f\"  Hour {hr:02d}: values = {valid_values}, average = {avg_val if pd.notna(avg_val) else 'None'}\")\n",
    "    \n",
    "    # Convert the dictionary to a list (order from hour 0 to 23)\n",
    "    rates_list = [profile_dict[h] for h in range(24)]\n",
    "    df_rates = pd.DataFrame({\"rate\": rates_list})\n",
    "    # Forward-fill then back-fill missing values\n",
    "    df_rates[\"rate\"] = df_rates[\"rate\"].ffill().bfill()\n",
    "    filled_profile = {h: df_rates.loc[h, \"rate\"] for h in range(24)}\n",
    "    \n",
    "    # Build the \"basalprofile\" array in the structure expected by netIOB.\n",
    "    basal_rate_structure_list = []\n",
    "    for hr in range(24):\n",
    "        rate_val = round(float(filled_profile[hr]), 3)\n",
    "        entry = {\n",
    "            \"i\": hr,\n",
    "            \"start\": f\"{hr:02d}:00:00\",\n",
    "            \"minutes\": hr * 60,\n",
    "            \"rate\": rate_val\n",
    "        }\n",
    "        basal_rate_structure_list.append(entry)\n",
    "    \n",
    "    # Construct the final profile dictionary.\n",
    "    profile_data = {\n",
    "        \"dia\": 5,\n",
    "        \"basalprofile\": basal_rate_structure_list\n",
    "    }\n",
    "    \n",
    "    # Save this daily profile as a JSON file.\n",
    "    profile_filename = os.path.join(profiles_folder, f\"profile_{day}.json\")\n",
    "    with open(profile_filename, \"w\") as f:\n",
    "        json.dump(profile_data, f, indent=2)\n",
    "    \n",
    "    daily_profiles[str(day)] = filled_profile\n",
    "    print(f\"Daily basal profile for {day} saved to {profile_filename}\")\n",
    "\n",
    "print(\"\\nDaily basal profiles created for:\")\n",
    "print(list(daily_profiles.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0b69b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oref0 checks\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Check if Node.js is installed by calling \"node -v\"\n",
    "try:\n",
    "    node_version = subprocess.check_output([\"node\", \"-v\"]).decode(\"utf-8\").strip()\n",
    "    print(\"Node.js is installed, version:\", node_version)\n",
    "except Exception as e:\n",
    "    print(\"Node.js does not appear to be installed. Please install Node.js to continue.\")\n",
    "    # Optionally, you can raise an error or exit:\n",
    "    # raise SystemExit(\"Node.js is required for netIOB computation.\")\n",
    "\n",
    "# Define the expected path to the oref0-calculate-iob.js script\n",
    "oref0_script_path = os.path.join(\"oref0\", \"bin\", \"oref0-calculate-iob.js\")\n",
    "\n",
    "# Check if the oref0 script exists at the expected location\n",
    "if os.path.exists(oref0_script_path):\n",
    "    print(\"Found oref0 script at:\", oref0_script_path)\n",
    "else:\n",
    "    print(f\"Error: oref0 script not found at expected location: {oref0_script_path}\")\n",
    "    # Provide a message or instructions on where to get it:\n",
    "    print(\"Please ensure you have cloned the oref0 repository from https://github.com/openaps/oref0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed9dbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the oref0 repository into a folder named 'oref0'\n",
    "!git clone https://github.com/openaps/oref0.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e820a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More oref0\n",
    "\n",
    "import os\n",
    "\n",
    "oref0_script_path = os.path.join(\"oref0\", \"bin\", \"oref0-calculate-iob.js\")\n",
    "if os.path.exists(oref0_script_path):\n",
    "    print(\"oref0 script successfully cloned at:\", oref0_script_path)\n",
    "else:\n",
    "    print(\"Error: oref0 script still not found at expected location:\", oref0_script_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68479259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install moment-timezone\n",
    "\n",
    "!cd \"oref0\" && npm install moment-timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e00f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# netIOB\n",
    "\"\"\"\n",
    "Methods – Computation of Net Insulin On Board (netIOB) Using Daily Basal Profiles\n",
    "\n",
    "The merged insulin dataset is first ingested from a CSV file containing 5‐minute insulin data. \n",
    "The dataset is sorted by its local timestamp and an additional ‘date_str’ column is derived by \n",
    "extracting the date component from the \"Local Time\" field. For each day, the data are processed \n",
    "individually. A day-specific basal profile JSON file, previously generated, is used as an input \n",
    "to the netIOB computation.\n",
    "\n",
    "For each day, the method iterates over all 5‐minute intervals and, for each interval, extracts the \n",
    "events that occurred in the prior 24 hours (using a helper function that filters the data based on \n",
    "timestamp). The filtered events are then formatted into a “pumphistory” JSON structure that matches \n",
    "the expected input format of the external netIOB computation script. The system creates a minimal \n",
    "clock file that contains the current 5‐minute interval’s timestamp (with timezone offset) and calls \n",
    "the Node.js script (oref0-calculate-iob.js) with the pump history, the corresponding day‐specific \n",
    "basal profile, and the clock file as parameters. The computed netIOB value is then stored in a \n",
    "dictionary indexed by the row number.\n",
    "\n",
    "After processing all rows for each day, the computed netIOB values are merged back into the original \n",
    "DataFrame, which is then exported as a CSV file containing the complete time series with an added \n",
    "\"netIOB\" column. This approach allows for day‐by‐day processing and enables partial output recovery \n",
    "if the computation is interrupted.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe30c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import subprocess\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from math import ceil\n",
    "import multiprocessing\n",
    "\n",
    "# Force the start method to 'fork' (only works on Unix-based systems)\n",
    "multiprocessing.set_start_method(\"fork\", force=True)\n",
    "\n",
    "# ---------------------------------------\n",
    "# CONFIGURATION\n",
    "# ---------------------------------------\n",
    "merged_file = \"insulin_5min_merged-input-netIOB.csv\"  # The 5-min insulin data\n",
    "profiles_folder = \"daily_profiles\"                    # Folder with day-specific profile JSON files\n",
    "oref0_script_path = \"oref0/bin/oref0-calculate-iob.js\"  # Path to the local oref0 script\n",
    "final_output_file = \"insulin_5min_merged-output-with-netIOB.csv\"\n",
    "\n",
    "# Can run different date series, eg test with 1-2 days first to make sure it works before running on the full series\n",
    "start_date = pd.to_datetime(\"2022-01-02\").date()\n",
    "end_date   = pd.to_datetime(\"2022-02-15\").date()\n",
    "\n",
    "# Create folders for partial outputs and temporary JSON files.\n",
    "partial_folder = \"partial_outputs\"\n",
    "os.makedirs(partial_folder, exist_ok=True)\n",
    "temp_dir = \"temp\"\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "# ---------------------------------------\n",
    "# HELPER FUNCTION: Build a Day-Long Pump History\n",
    "# ---------------------------------------\n",
    "def build_day_history(df_all, day_str):\n",
    "    \"\"\"\n",
    "    Build a pumpHistory list for a given day.\n",
    "    The history includes all events from (day_start - 24h) to (day_end).\n",
    "    Returns a list of dictionaries in the oref0 pumpHistory format.\n",
    "    IMPORTANT: The events are sorted in descending order (most recent first)\n",
    "    as expected by the oref0 netIOB calculation.\n",
    "    \"\"\"\n",
    "    day_start = pd.Timestamp(day_str)\n",
    "    day_end = day_start + timedelta(days=1)\n",
    "    # Filter for events in the lookback window up to the end of the day.\n",
    "    history_df = df_all[(df_all[\"Local Time\"] >= (day_start - timedelta(hours=24))) & \n",
    "                        (df_all[\"Local Time\"] < day_end)].copy()\n",
    "    # Sort in descending order (most recent first)\n",
    "    history_df = history_df.sort_values(\"Local Time\", ascending=False)\n",
    "    \n",
    "    pumphistory_data = []\n",
    "    for _, row in history_df.iterrows():\n",
    "        hist_time_str = row[\"Local Time\"].strftime(\"%Y-%m-%dT%H:%M:%S+00:00\")\n",
    "        ttype = str(row[\"Tidepool Data Type\"]).lower()\n",
    "        if ttype == \"basal\":\n",
    "            duration_mins = float(row[\"Duration (mins)\"]) if not pd.isna(row[\"Duration (mins)\"]) else 0.0\n",
    "            pumphistory_data.append({\n",
    "                \"timestamp\": hist_time_str,\n",
    "                \"_type\": \"TempBasalDuration\",\n",
    "                \"duration (min)\": duration_mins\n",
    "            })\n",
    "            basal_rate = float(row[\"Temp rate\"]) if not pd.isna(row[\"Temp rate\"]) else 0.0\n",
    "            pumphistory_data.append({\n",
    "                \"timestamp\": hist_time_str,\n",
    "                \"_type\": \"TempBasal\",\n",
    "                \"temp\": \"absolute\",\n",
    "                \"rate\": round(basal_rate, 3)\n",
    "            })\n",
    "        elif ttype == \"bolus\":\n",
    "            bolus_amt = float(row[\"Amount\"]) if not pd.isna(row[\"Amount\"]) else 0.0\n",
    "            pumphistory_data.append({\n",
    "                \"timestamp\": hist_time_str,\n",
    "                \"_type\": \"Bolus\",\n",
    "                \"amount\": bolus_amt,\n",
    "                \"programmed\": bolus_amt,\n",
    "                \"unabsorbed\": 0,\n",
    "                \"duration\": 0\n",
    "            })\n",
    "    return pumphistory_data\n",
    "\n",
    "# ---------------------------------------\n",
    "# HELPER FUNCTION: Compute netIOB for a Single Timestamp (Worker Function)\n",
    "# ---------------------------------------\n",
    "def compute_iob_for_timestamp(current_time, history_file, day_profile_file, temp_dir, oref0_script_path):\n",
    "    \"\"\"\n",
    "    Writes a unique clock file for the current_time, calls the oref0 Node.js script,\n",
    "    and returns the computed netIOB value.\n",
    "    \"\"\"\n",
    "    clock_str = current_time.strftime(\"%Y-%m-%dT%H:%M:%S+00:00\")\n",
    "    unique_clock_file = os.path.join(temp_dir, f\"clock_{current_time.timestamp()}_{os.getpid()}.json\")\n",
    "    with open(unique_clock_file, \"w\") as cf:\n",
    "        json.dump(clock_str, cf)\n",
    "    try:\n",
    "        output = subprocess.check_output([\n",
    "            \"node\",\n",
    "            oref0_script_path,\n",
    "            history_file,        # full pump history file\n",
    "            day_profile_file,      # day-specific profile\n",
    "            unique_clock_file\n",
    "        ])\n",
    "        output_json = json.loads(output)\n",
    "        iob_val = output_json[0][\"iob\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing netIOB for time {current_time}: {e}\")\n",
    "        iob_val = 0.0\n",
    "    os.remove(unique_clock_file)\n",
    "    return iob_val\n",
    "\n",
    "# ---------------------------------------\n",
    "# HELPER FUNCTION: Process netIOB for a Single Day Using Multiprocessing\n",
    "# ---------------------------------------\n",
    "def compute_netiob_for_day(day_data, history_file, day_profile_file, temp_dir, oref0_script_path):\n",
    "    times = day_data[\"Local Time\"].tolist()\n",
    "    args = [(t, history_file, day_profile_file, temp_dir, oref0_script_path) for t in times]\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        results = pool.starmap(compute_iob_for_timestamp, args)\n",
    "    return results\n",
    "\n",
    "# ---------------------------------------\n",
    "# MAIN FUNCTION: PROCESS netIOB Iteratively by Day (Test Range)\n",
    "# ---------------------------------------\n",
    "def compute_netiob_daily_profiles_iterative():\n",
    "    df = pd.read_csv(merged_file, parse_dates=[\"Local Time\"])\n",
    "    df.sort_values(\"Local Time\", inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df[\"date_str\"] = df[\"Local Time\"].dt.date.astype(str)\n",
    "    \n",
    "    all_dates = sorted(df[\"date_str\"].unique())\n",
    "    filtered_dates = [d for d in all_dates if pd.to_datetime(d).date() >= start_date \n",
    "                      and pd.to_datetime(d).date() <= end_date]\n",
    "    \n",
    "    output_dfs = []\n",
    "    processed_days = []\n",
    "    \n",
    "    for day_str in filtered_dates:\n",
    "        start_day_time = datetime.now()\n",
    "        print(f\"\\nProcessing date {day_str}... (start: {start_day_time.strftime('%Y-%m-%d %H:%M:%S')})\")\n",
    "        \n",
    "        day_data = df[df[\"date_str\"] == day_str].copy()\n",
    "        # Build pump history (lookback from day_start - 24h to day_end)\n",
    "        history = build_day_history(df, day_str)\n",
    "        history_file = os.path.join(temp_dir, f\"pumphistory_{day_str}.json\")\n",
    "        with open(history_file, \"w\") as phf:\n",
    "            json.dump(history, phf, indent=2)\n",
    "        print(f\"  Pump history for {day_str} written to {history_file} with {len(history)} events.\")\n",
    "        \n",
    "        day_profile_file = os.path.join(profiles_folder, f\"profile_{day_str}.json\")\n",
    "        if not os.path.exists(day_profile_file):\n",
    "            print(f\"Warning: No profile file for {day_str}. Setting netIOB=0 for all rows.\")\n",
    "            netiob_list = [0.0] * len(day_data)\n",
    "        else:\n",
    "            netiob_list = compute_netiob_for_day(day_data, history_file, day_profile_file, temp_dir, oref0_script_path)\n",
    "        \n",
    "        day_data[\"netIOB\"] = netiob_list\n",
    "        partial_file = os.path.join(partial_folder, f\"partial_{day_str}_with_netiob.csv\")\n",
    "        day_data.to_csv(partial_file, index=False)\n",
    "        print(f\"  Saved partial output for {day_str} to {partial_file}\")\n",
    "        \n",
    "        end_day_time = datetime.now()\n",
    "        duration = end_day_time - start_day_time\n",
    "        print(f\"Finished processing {day_str} at {end_day_time.strftime('%Y-%m-%d %H:%M:%S')}. Duration: {duration}\")\n",
    "        \n",
    "        output_dfs.append(day_data)\n",
    "        processed_days.append(day_str)\n",
    "    \n",
    "    final_df = pd.concat(output_dfs, ignore_index=True)\n",
    "    final_df.to_csv(final_output_file, index=False)\n",
    "    print(f\"\\nAll done. Final netIOB data saved to {final_output_file}.\")\n",
    "    print(\"Processed days:\", processed_days)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    compute_netiob_daily_profiles_iterative()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
